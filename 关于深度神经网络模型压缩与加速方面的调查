视频版本请访问：https://www.bilibili.com/video/BV1Z341127am/
本文源自：https://arxiv.org/abs/1710.09282

我的观点：
本文全面概述了深度神经网络模型的压缩方法，主要可分为参数剪枝与共享，低秩分解，迁移/压缩卷积滤波器和知识蒸馏，该文章针对每一类方法的性能、相关应用、优势和劣势进行了分析，并且介绍了模型压缩主流benchmark 以及未来展望。适合入门选手阅读，本文就以原有文章的组织顺序介绍相关内容。
概述：
深度卷积神经网络(CNNs)最近已经在许多视觉识别任务上取得了巨大的成功。然而，现存的深度卷积神经网络模型在算力和内存的要求很高，这限制了它们在低内存或者应用的部署。因此，一个自然的想法式就是再没有模型性能上的显著下降下使用模型压缩和加速深度网络。在过去几年里，这个领域取得了巨大的（tremendous）进展。这篇文章中，我们将会调查用于紧凑和加速CNNs模型的最先进技术。这些技术可以笼统的分成四个大类：参数剪枝和共享（parameter pruning and sharing），低秩分解（low-rank factorization），变换/紧致卷积滤波器（transferred/compact convolutional filters），以及知识蒸馏（knowledge distillation）在每一部分，我们提供细致分析在性能，相关应用，优点和缺点。然后我们将通过一些最近成功的方法，例如，动态容量网络(dynamic capacity networks)和随机深度网络(stochastic depths networks)。这之后，我们调查评估矩阵（evaluation matrices,）最主要的用于评估模型性能的数据集和最近的基准效果。最后，我们总结这个文章，讨论剩下的挑战和这个主题（模型压缩）可能的方向。
介绍（一睹为快）：
方法	描述	应用场景	优势	缺点	其它
参数剪枝与共享	减少对性能不敏感的冗余参数	卷积层和全连接层	对不同设置都具有健壮性，能获得一个好的性能，同时支持脚本训练和预训练模型	极端情况二值化网络会损失精度	
低秩分解	使用矩阵/向量分解去评估带有信息量的参数	卷积层和全连接层	标准的管道，更容易实现，同时支持脚本训练和预训练模型	有时反而会增加计算代价，不能够保持全局信息	
转移/紧凑卷积核	设计要给特殊的空间卷积核以节约参数	卷积层	算法依赖于应用，通常获得好的性能，但只支持脚本训练	诸如在Googlenet，Resnet上的表现不太好	
知识蒸馏	从大模型中蒸馏出一个紧凑型神经神经网络	卷积层核全连接层	模型的性能对应用比较敏感，即泛化能力弱。只支持脚本训练	只能应用到带有softmax损失的分类任务	
					
一、参数剪枝与共享
A.量化与二值化
量化：使用更少的bit位数来压缩原有模型中的权值，一般情况下训练出来的网络模型权重都是采用float表示的，例如将其经过定点量化后采用定点8bit或16bit表示权重。
二值化：二值化网络是量化的极端操作，即使用1bit表示权重，虽然大幅度减少了权重占用空间，但是实验表明对模型的性能影响也比较大。


Han S 提出一套完整的深度网络的压缩流程：首先修剪不重要的连接，重新训练稀疏连接的网络。然后使用权重共享量化连接的权重，再对量化后的权重和码本进行霍夫曼编码，以进一步降低压缩率。如图 2 所示，包含了三阶段的压缩方法：修剪、量化（quantization）和霍夫曼编码。
B.网络剪枝
网络剪枝和共享已经被用于减少网络复杂性和处理过拟合任务。一个早些的剪枝方法是基于权值的。最优脑损害(The Optimal Brain Damage)和最优脑外科医生(the Optimal Brain Surgeon)方法减少了基于损失函数Hessian阵的连接层的参数，这些工作说明这样的剪枝得到了较高的精确率比那些基于权值剪枝方法的机制。这个方向的最近趋势是剪枝冗余性，即预训练CNN模型中非信息的权值。训练紧致的带有稀疏约束的CNNs同样也是一个增长的兴趣点。这些稀疏性约束时典型的引入了最优化问题如 [公式] 或者 [公式] 范数正则化。缺点： 剪枝和共享有一些潜在的问题。首先，带有 [公式] 或者 [公式]正则化的剪枝更多的迭代以达到收敛（难以收敛）。另外，所有的剪枝的关键点时要求对层的敏感性的手动启动，这要求微调好参数因此在一些应用上可能变得更加笨重。
C.设计矩阵结构（针对全连接层）
我们知道全连接层通常携带了大量的参数，是网络中的内存带宽与计算的瓶颈。在这个方向上，有一些研究提出了简单而有效的基于循环投影的方法，给一定一个向量和一个循环矩阵，如下：

这样内存消耗就会从o(n²）减少为o(n).这种方法的缺点是：1.有可能会影响模型的性能 2.必须有通过手动方式去设计一个合适的矩阵结构。目前没有没有通用的理论支持。
二、低秩分解
卷积操作占据了深度CNNs模型中的最大计算篇幅，因此减少卷积层能够提升压缩率同时也能加快运算速度。对于卷积核，它可以被认为一个4D张量。基于张量分解的想法驱使我们本能的认为4D张量的冗余性去除会有一个显著提升，这是一个特殊的方式去移除冗余性。注意到全连接层，它能够被视为一个2D矩阵并且低秩也能有所帮助。

很长时间以来，人们使用低秩滤波器去加速卷积，例如，高维DCT（discrete cosine transform, 离散余弦变换）和小波系统（wavelet systems）各自使用张量积去构造从1D DCT和1D的小波。（。。。一些具体的方法）低秩估计是逐层而做的。一层的参数在低质估计后固定下来，而上面的层是微调基于一个重构误差准则。这些是典型的用于压缩2D卷积滤波器的低秩估计方法。根据这个方向，Canonical Polyadic(CP)分解是一个用于核张量的方法。人们使用非线性的最小二乘去计算CP分解。然而，找到最好的低秩估计在CP分解中式一个病态问题，并且最好的 [公式] （K是秩的值）估计可能是不存在的。。

正如前面所提，全连接层可以看成是2D矩阵，因此上述的方法（指低秩估计的方法）也能够应用到这儿（指全连接层的分解）。也有工作应用截断奇异值分解去分解全连接层用于设计紧凑的多任务深度学习框架。

缺点：低秩方法在估计矩阵压缩和加速上是简单直接的。这个想法最近补充深度学习的优点，比如dropout, rectified units 和 maxout. 然而， 由于它涉及到分解操作，执行起来并不容易。分解是计算代价昂贵的。 另一个问题是当前的低秩估计方法是逐层的，因此不能执行全局的参数压缩， 而这对于不同的层保持不同的信息时重要的。最后，比较原始模型， 分解需要额外的再训练去实现收敛。

三、转移、紧凑※卷积核
CNNs 是参数有效的对于转换的不变性，对于输入图片表示的不变性，而这对于训练深度模型而不导致过拟合是重要的。尽管目前还缺失强理论证明，但是大量的经验事实支持转换的不变性和卷积权值共享对于好的预测性能是重要的。 使用转移卷积滤波器去压缩CNN模型的想法受到研究工作启发，它引入了等价理论。

四、知识蒸馏
就我们所知道的，第一次探索使用知识转移的方法压缩模型的工作是由Caruana等人提出的。他们训练一个压缩/完整的带有伪数据标签的强分类器的模型，并复现了原始大网络的效果。但是这项工作被限制到了浅层模型。这个想法最近吸收为知识蒸馏(Knowledge Distillation)压缩深和宽的网络到浅的网络。 知识蒸馏的基本思想是去转移大的教师模型的知识到一个小的学生模型通过学习由softmax带来的类分布。

缺点：基于知识蒸馏的方法能够使更深的模型瘦身，并且显著的减少计算损失。然而，知识蒸馏只能应用到带有softmax损失的分类任务，这限制了其应用，另一个缺点使模型假设有时过于严格，相比较其他竞争性的方法。
五、讨论与挑战
	· 依赖于原模型，降低了修改网络配置的空间，对于复杂的任务，尚不可靠；
	· 通过减少神经元之间连接或通道数量的方法进行剪枝，在压缩加速中较为有效。但这样会对下一层的输入造成严重的影响；
	· 结构化矩阵和迁移卷积滤波器方法必须使模型具有较强的人类先验知识，这对模型的性能和稳定性有显著的影响。研究如何控制强加先验知识的影响是很重要的；
	· 知识精炼方法有很多优势，比如不需要特定的硬件或实现就能直接加速模型。个人觉得这和迁移学习有些关联。
	· 多种小型平台（例如移动设备、机器人、自动驾驶汽车）的硬件限制仍然是阻碍深层 CNN 发展的主要问题。相比于压缩，可能模型加速要更为重要，专用芯片的出现固然有效，但从数学计算上将乘加法转为逻辑和位移运算也是一种很好的思路。
参考文献：
https://zhuanlan.zhihu.com/p/137117572

https://www.jianshu.com/p/885bee2a4aa
